# Fundamentals of computational data analysis using R
## Machine learning: partitioning methods
#### Contact: mitch.kostich@jax.org

---

### Index

- [Trees](#trees)
- [Random forest](#random-forest)
- [Boosting](#boosting)

### Check your understanding

- [Check 1](#check-your-understanding-1)
- [Check 2](#check-your-understanding-2)
- [Check 3](#check-your-understanding-3)

---

### Trees

Decision trees are a way of making predictions about continuous (regression) or categorical
  (classification) response variables based on a series of `if()` conditionals. These 
  conditionals are organized in a tree-like hierarchy, with the sequence of conditionals applied
  to an observation being a function of the predictor values for that observation. Each 
  conditional can be represented as a node in a tree. The outcome of the conditional test at 
  a node determines which branch coming out of that node the observation will be passed to. Each 
  conditional typically involves a test on a single variable and produces one of two outcomes, 
  such as `TRUE` or `FALSE`. So each node produces a two-way or binary split. Trees tend to be
  a great way of representing the logic used to assign response values. However, they tend to
  perform poorly for prediction. Although decision trees can be unbiased if they are allowed
  to grow large enough, they tend to have very high variance due to a tendency to overfit the
  training-set.

Fitting a tree involves deciding which variables to use for splitting, the cutoff to use for
  splitting, and the optimal positions within the tree for each split. The way this is usually 
  done in practice is to successively split the data in a way that decreases some sort of loss
  metric. For classification, fitting aims to decrease the class **impurity** of the set of
  observations in each of the two branches coming out of the node relative to the impurity 
  measure of the set of observations coming into the node. There are several possible metrics 
  for impurity. They are similar in that they all assign a loss of zero when the the set of
  observations includes only one class, but differ in how severely they penalize mixed class
  compositions. One very commonly used metric is the **Gini Index**, `f(p.i) = p.i * (1 - p.i)`, 
  where `p.i` is proportion of observations input to the node that belong to class `i`. 
  Observations w/ missing values for the split variable are not counted in the impurity 
  calculation. Node impurity is `sum.over.i(f(p.i))`, or the sum of class impurities. For 
  regression, the loss is typically something like the mean-squared-error. Tree 
  fitting proceeds one node at a time, finding the variable and split value that most minimizes 
  the loss in each of the two branches coming out of the node. The process terminates when
  some minimal node size or level of node loss is achieved. This results in a **leaf node** or 
  **terminal node**, which is where the final response value prediction value is assigned. 
  For classification, the most frequently occurring class among the training-set passing to the
  terminal node is assigned to any new observations that arrive at this node. For regression, 
  the mean of the response in the training-set observations that passed into this node is used
  as the predicted value for new observations destined for this node.

In the example below, we with use the `rpart::rpart()` function for fitting. The fitting process
  is controlled by an object generated by the `rpart::rpart.control()` function. This controls
  the complexity of the tree as well as parameter tuning using cross-validation. The primary 
  `rpart()` argument to be tuned is `cp`, or the **complexity parameter**. This is a numeric 
  value that specifies by what proportion a new node split must decrease the loss function in 
  order to be considered for addition to the tree. the `rpart()` function carries out the 
  cross-validation (with fold-number set by the `xval` argument, which defaults to `10`). 
  Another argument used for limiting tree size (and therefore model complexity) is `min.split`, 
  which is the minimum number of training observations that must be assigned to a node for 
  further splits along that path to be considered. In addition, we can limit the minimum 
  number of training-set observations assigned to a leaf using the `min.bucket` argument. If 
  a split would result in a child node smaller than `min.bucket`, the split will not be 
  attempted. We can also limit the maximum tree depth using the `maxdepth` argument, which 
  limits the total number of conditionals that can be applied in series to any single 
  observation.

```
library(rpart)
library(caret)

rm(list=ls())

## reformat prostate cancer recurrence dataset:
?rpart::stagec
dat <- rpart::stagec
dat$pgstat <- c('no', 'yes')[dat$pgstat + 1]
dat$pgstat <- factor(dat$pgstat)  ## character -> factor
dat$pgtime <- NULL                ## drop column
summary(dat)                      ## note the NAs (missing values)
head(dat)
nrow(dat)

## split into training and test-set in way amenable to CV:
set.seed(1)
idx <- 1:nrow(dat)
folds <- caret::createMultiFolds(idx, k=5, times=12)
idx.trn <- folds[[1]]
dat.trn <- dat[idx.trn, ]
dat.tst <- dat[-idx.trn, ]
summary(dat.trn)
summary(dat.tst)

## take a peak:
par(mfrow=c(1, 1))
plot(dat.trn)

## fit the model, tuning 'cp' complexity parameter by (10-fold) CV:
fit0 <- rpart(pgstat ~ ., data=dat.trn, method='class')
fit0
class(fit0)
is.list(fit0)
names(fit0)
summary(fit0)
plot(fit0)                        ## plot the tree
text(fit0)                        ## add node labels
plotcp(fit0)                      ## plot cross-validation
printcp(fit0)                     ## print cross-validation (xerror is CV error)
(tbl <- fit0$cptable)

## prune tree using 'cp' value chosen from tuning:
(idx.bst <- which.min(tbl[, 'CP']))
tbl[idx.bst, ]
(fit <- prune(fit0, cp=tbl[idx.bst, 'CP']))
plot(fit)
text(fit, cex=0.75)

## make (probabilistic) predictions and take a look:
(prd.tst <- predict(fit, newdata=dat.tst, type='prob'))
prd.tst <- prd.tst[, 'yes']
roc.tst <- pROC::roc(dat.tst$pgstat == 'yes', prd.tst, direction='<')
roc.tst$auc
pROC::ci.auc(roc.tst)
(prd.class.tst <- c('no', 'yes')[(prd.tst > 0.5) + 1])
## mcnemar's test compares sensitivity and specificity:
(cnf.tst <- caret::confusionMatrix(dat.tst$pgstat, factor(prd.class.tst)))

## do predictions work better on training set?
prd.trn <- predict(fit, newdata=dat.trn, type='prob')
prd.trn <- prd.trn[, 'yes']
roc.trn <- pROC::roc(dat.trn$pgstat == 'yes', prd.trn, direction='<')
roc.trn$auc
pROC::ci.auc(roc.trn)
prd.class.trn <- c('no', 'yes')[(prd.trn > 0.5) + 1]
(cnf.trn <- caret::confusionMatrix(dat.trn$pgstat, factor(prd.class.trn)))

```

[Return to index](#index)

---

### Check your understanding 1

Starting with the following:

```
library(rpart)
library(caret)

rm(list=ls())

## reformat prostate cancer recurrence dataset:
dat <- rpart::stagec
dat$pgstat <- c('no', 'yes')[dat$pgstat + 1]
dat$pgstat <- factor(dat$pgstat)  ## character -> factor
dat$pgtime <- NULL                ## drop column

```

Using 5-fold cross-validation repeated 12 times, generate a point estimate of the AUC
  for an `rpart` classification tree with formula `pgstat ~ .` where the `cp` complexity
  parameter is chosen using an inner cross-validation loop. Hint: you don't need to
  do the inner cross-validation explicitly -- `rpart()` does it for you.

[Return to index](#index)

---

### Random forest

A tree model's prediction error is largely attributable to high variance rather than bias, 
  and the model response variable predictions are non-linear functions of the predictors. 
  The performance of classifiers with these attributes can often be substantially improved
  through **bagging**. Bagging, or **boostrap aggregation** describes fitting a separate 
  model to a boostrap sample of the original training-set observations. That is, we sample 
  the training-set observations with replacement. We then fit a separate tree model to each 
  of the bootstrap samples. The set of tree models, forms an **ensemble** or **committee** 
  of models, each of which is used to make predictions for a new observation. The final 
  prediction for that new observation is determined by either plurality vote (in the case 
  of classification with a categorical response and discrete class prediction) or by 
  averaging (in the case of regression with a continuous response variable or probabilistic
  predictions of class membership). The ensemble model's variance is expected to be reduced 
  relative to the original model, but the bias is expected to be the same, so overall, the 
  ensemble model should on average have higher accuracy than a single tree.

One byproduct of the bagging process, is that for each tree, there will tend to be some 
  observations that do not appear in the bootstrap sample used to train that tree. These 
  observations can serve as independent test data for estimating the performance of that
  individual tree. These performance estimates are referred to as **out-of-bag** or
  **OOB** estimates.

The bagging process does not work very well when the predictions of different classifiers in 
  the ensemble are highly correlated to one another. This is one reason that bagging tends 
  to work better for non-linear classifiers than linear ones: linear classifiers built on 
  different bootstrap samples of the same dataset tend to be more correlated with one another 
  than non-linear classifiers will be. In the case of trees, we can decorrelate the trees built 
  using different bootstrap samples by only considering a randomly selected subset of features 
  of size `mtry` for splitting at each node within each tree. This random feature sampling at 
  each node tends to induce major topological differences and differences in variables used at 
  each split across the different trees in the ensemble, drastically reducing correlations 
  between trees at the expense of potentially introducing some bias. The trade-off between these 
  two effects is controlled by tuning the number of features `mtry` randomly selected at each 
  node. If there are very few informative features, the optimum `mtry` will tend to be larger 
  so that most node fits are likely get to consider a useful feature and result in a productive 
  split. By contrast, if there are many informative features, smaller `mtry` results in feature 
  sets that are still relatively likely to include useful features, so we can take advantage of 
  the better decorrelation offered by smaller feature sets without affecting the performance of 
  individual trees. The higher the proportion of correlated features, the higher the correlation 
  between predictions between different trees. Under these circumstances, the higher decorrelation 
  offered by smaller feature sets results in better overall performance of the ensemble. In 
  general, wherever practical, we will tune the `mtry` parameter by cross-validation. 

**Random forest** is an ensemble modeling approach built on recursive partitioning tree models,
  where bagging is used to generate the series of models, and random feature selection at each
  node within each tree is used to decorrelate trees in the ensemble from one another, improving
  the variance reduction afforded by the bagging. Ensemble methods are often criticized for 
  trading away interpretability for improved predictive performance. The models do tend to be
  very complex, but there are several tools that help us determine both the importance of
  individual features for the performance of the ensemble, as well as the relationship between
  the response predictions and individual variables. 

**Variable importance** is usually estimated in one of two ways. The first uses the magnitude of 
  reduction in loss (purity or sums-of-squares) resulting from splitting a variable in one tree 
  contributes to the importance of the variable. By summing the importance across all the nodes 
  involving this variable across all trees in the ensemble, we can express an overall importance. 
  Another approach is to use an 'out-of-bag' estimate of the prediction error: for each tree and 
  each variable, we can calculate the OOB error using the original data, then with the variable of 
  interest permuted (so it has no relationship to the response any more). The change in performance, 
  averaged across all the trees in the forest, is attributed to the permuted variable. Smaller values 
  for `mtry` tend to make variable importances more similar to one another, because it tends to give 
  weaker variables a larger chance of being selected at each split, because there is less chance that 
  the variable options will include one of the stronger variables which would otherwise be selected.

We can also get a feel for the 'shape' of the relationship between an individual feature (say `x.i`)
  and the response prediction using **partial dependence** plots. To generate such a plot, we take each 
  observation in the training set, set the value of `x.i` to a particular value, while leaving the
  rest of the features unchanged, and generate a response prediction for the modified observation using
  the previously fit random forest model. By doing this for all observations across a sequence of `x.i` 
  values, we can see how changing `x.i` affects predictions on its own. We use the 
  `randomForest::partialPlot()` function for this purpose.

```
library(caret)
library(pROC)
library(glmnet)
library(randomForest)

## data:

rm(list=ls())
data(dhfr)                        ## from caret
dat <- dhfr
class(dat)
dim(dat)
table(dat$Y)
class(dat$Y)

## generate hold-out test-set:

set.seed(1)
idx <- 1 : nrow(dat)
folds <- caret::createMultiFolds(idx, k=5, times=3)
idx.trn <- folds[[1]]

dat.trn <- dat[idx.trn, ]
dat.tst <- dat[-idx.trn, ]
names(dat.trn)[1]

## mtry tuning using out-of-bag error:

(tune.fit <- tuneRF(x=dat.trn[, -1], y=dat.trn$Y, stepFactor=0.5, improve=0.01, 
  ntreeTry=1000, trace=T, plot=T, doBest=F, replace=T))
class(tune.fit)
(score.best <- min(tune.fit[, 'OOBError']))
(i.best <- tune.fit[, 'OOBError'] == score.best)
(mtry.best <- min(tune.fit[i.best, 'mtry']))

## fit with selected mtry:

fit <- randomForest(x=dat.trn[, -1], y=dat.trn$Y, mtry=mtry.best, ntree=1000, importance=T, replace=T)
par(mfrow=c(1, 1))
plot(fit, log='y')

## probabilistic predictions:

prd.tst <- predict(fit, newdata=dat.tst[, -1], type='prob')
prd.trn <- predict(fit, newdata=dat.trn[, -1], type='prob')

head(prd.tst)
prd.tst <- prd.tst[, 'active']
prd.trn <- prd.trn[, 'active']

## evaluation: 

roc.tst <- pROC::roc(dat.tst$Y == 'active', prd.tst, direction='<')
roc.tst$auc
pROC::ci.auc(roc.tst)
prd.class.tst <- c('inactive', 'active')[(prd.tst > 0.5) + 1]
(cnf.tst <- caret::confusionMatrix(dat.tst$Y, factor(prd.class.tst)))

roc.trn <- pROC::roc(dat.trn$Y == 'active', prd.trn, direction='<')
roc.trn$auc
pROC::ci.auc(roc.trn)
prd.class.trn <- c('inactive', 'active')[(prd.trn > 0.5) + 1]
(cnf.trn <- caret::confusionMatrix(dat.trn$Y, factor(prd.class.trn)))

## importance: type=2 is average (over trees) decrease in node impurity achieved by splitting on variable

imp.trn <- randomForest::importance(fit, type=2)
imp.trn[1:30, , drop=F]
imp.trn <- imp.trn[order(imp.trn, decreasing=T), , drop=F]
imp.trn[1:5, ]
par(mfrow=c(1, 1))
randomForest::varImpPlot(fit, type=2, mex=0.75, cex=0.75)

## partial dependence plot:

rownames(imp.trn)[1:6]
par(mfrow=c(2, 3))
for(idx in 1:6) {
  (x.var1 <- rownames(imp.trn)[idx])
  randomForest::partialPlot(fit, pred.data=dat.trn, x.var=c(x.var1), which.class='active', main=x.var1)
}

```

[Return to index](#index)

---

### Check your understanding 2

Use the `dhfr` data from the `caret` package to perform 5-fold cross-validation repeated twice to 
  estimate the AUC for a model with `Y` as categorical response and the rest of the features as
  predictors. Tune the `mtry` parameter using the `tuneRF()` function, specifying `stepFactor=0.5` 
  and `improve=0.01`.

[Return to index](#index)

---

### Boosting

intro here;

Boosted trees are harder to tune, but sometimes have better performance than random forest.
  At the cost of increased complexity of tuning and computational load.

Weights observations based on how hard they are to classify (for categorical response) or 
  by how big the residual is (for continuous response). Initially all 1/n. Iteratively 
  fit model, then reweight observations based on results. Repeat a user-specified number 
  of times. The differences in approaches are largely related to how 
  the weights are updated. The simpler to understand AdaBoost.M1 algorithm is described
  below:

for classifier AdaBoost.M1: in adabag, fastadaboost
  step1: wts.obs <- 1/n
  step2: repeat M times:
    a) fit model
    b) compute model error err as weighted (wts.obs) average of observation errors
    c) compute wt adjustment adj <- log((1 - err)/err); this controls the 'step-size' for
         the change in weighting for this iteration
    d) update weights: wts <- wts * exp(adj * as.numeric(y == f(x))
  step3: output sum.m(adj.m * f.m(x)) for all M models

The effect of including all M models in the final committee is equivalent to fitting
  the first model to the data, then the second to the residuals from the first, and the
  third to the residuals of the second, etc.

gradient boosting: adjusts the weights in a more complex and effective way; 
  for regression, gradient boosting in gbm. Find direction in the weights space (where
  each variable weight defines an orthogonal dimension) which most decreases loss
  function (estimated from training-set). For instance, for regression with squared-error
  loss function (not the only choice), find direction in weight space that most reduces the 
  MSE mean(fitted - observed) for training set.
  Take a step in that direction of a size determined by a 'learning rate' parameter.
  Setting learning-rate < 1 is like shrinkage of step size, a regularization of sorts that
  reduces chance of overfitting, but also increases number of steps needed to converge. 
  Tune learning-rate vs. number of iterations. Can pick learning-rate that is as small as
  tolerable given time/compute bandwidth/ram, then tune number of iterations by CV. Can
  sample observations w/ replacement at each step in order to reduce overfitting.  
  Also lower learning-rate benefits from more trees.

```
library(caret)
library(pROC)
library(gbm)

## data:

data(dhfr)                        ## from caret
dat <- dhfr
class(dat)
dim(dat)
table(dat$Y)
class(dat$Y)                      ## need this to be 0/1

dat$Y <- as.numeric(dat$Y == 'active')
table(dat$Y)
class(dat$Y)

## generate hold-out test-set:

set.seed(1)
idx <- 1 : nrow(dat)
folds <- caret::createMultiFolds(idx, k=5, times=3)
idx.trn <- folds[[1]]

dat.trn <- dat[idx.trn, ]
dat.tst <- dat[-idx.trn, ]

fit <- gbm::gbm(Y ~ ., data=dat.trn, distribution="bernoulli", 
  n.trees=1000, shrinkage=0.01, interaction.depth=3, n.minobsinnode=10, 
  cv.folds=5, keep.data=F, verbose=T, n.cores=1)

fit
class(fit)
is.list(fit)
names(fit)

## best stopping point; in plot, training error black, cv error green:

(n.trees.best <- gbm.perf(fit, method="cv"))
summary(fit, n.trees=1)                ## first tree
summary(fit, n.trees=n.trees.best)     ## final series of trees

## probabilistic predictions:

(prd.tst <- predict(fit, newdata=dat.tst, n.trees=n.trees.best, type="response"))
prd.trn <- predict(fit, newdata=dat.trn, n.trees=n.trees.best, type="response")

## evaluate:

roc.tst <- pROC::roc(dat.tst$Y, prd.tst, direction='<')
roc.tst$auc
pROC::ci.auc(roc.tst)
prd.class.tst <- (prd.tst > 0.5) + 1
(cnf.tst <- caret::confusionMatrix(factor(dat.tst$Y + 1), factor(prd.class.tst)))

roc.trn <- pROC::roc(dat.trn$Y, prd.trn, direction='<')
roc.trn$auc
pROC::ci.auc(roc.trn)
prd.class.trn <- (prd.trn > 0.5) + 1
(cnf.trn <- caret::confusionMatrix(factor(dat.trn$Y + 1), factor(prd.class.trn)))

## importance plots, can be multivariate; can specify variables by integer index or name:

smry <- summary(fit, n.trees=n.trees.best)
smry$var[1:6]

plot(fit, i.var=smry$var[1], n.trees=n.trees.best)  
plot(fit, i.var=smry$var[2], n.trees=n.trees.best)  
plot(fit, i.var=smry$var[3], n.trees=n.trees.best)  
plot(fit, i.var=smry$var[4], n.trees=n.trees.best)  

```

[Return to index](#index)

---

### Check your understanding 3

Starting with the following data:

```
library(rpart)

rm(list=ls())

## reformat prostate cancer recurrence dataset:
dat <- rpart::stagec
dat$pgtime <- NULL                ## drop column

```

Use 5-fold cross-validation, repeated three times to estimate the AUC of a gradient boosted 
  tree model, built using `gbm()` with `shrinkage=0.01`, `interaction.depth=2`, and 
  `n.minobsinnode=5`. Use an inner 5-fold cross-validation loop to select the number of
  iterations. Hint: the `gbm()` function does the inner-loop parameter tuning for you.

[Return to index](#index)

---

## FIN!
