# Fundamentals of computational data analysis using R
## Machine learning: partitioning methods
#### Contact: mitch.kostich@jax.org

---

### Index

- [Decision trees](#decision-trees)
- [Random forest](#random-forest)
- [Boosting](#boosting)

### Check your understanding

- [Check 1](#check-your-understanding-1)
- [Check 2](#check-your-understanding-2)
- [Check 3](#check-your-understanding-3)

---

### Decision trees

Decision trees are an algorithm for explaining or making predictions about continuous (regression) 
  or categorical (classification) response variables based on a series of `if()` conditionals. Each 
  conditional can be represented as a node in a tree. The outcome of the conditional test at 
  a node determines which branch coming out of that node the observation will be passed to, and 
  therefore which other conditionals will be applied to it. Each conditional typically involves a 
  test on a single variable and produces one of two outcomes, such as `TRUE` or `FALSE`. So each 
  node produces a two-way or binary split. Trees tend to be a great way of representing the logic 
  used to assign response values. However, they tend to perform poorly for prediction. Although 
  decision trees can be unbiased as long as they are allowed to grow large enough, they tend to 
  have very high variance due to overfitting (capturing the noise in) the training-set.

Fitting a tree involves deciding which variables to use for splitting, the cutoff to use for
  splitting, and the optimal positions within the tree for each splitting decision. The fitting
  proceeds by adding one node (conditional) to the tree at a time. The choice of variable and cutoff 
  used by the node is made so as to minimize some sort of **loss function**, that is, a function that 
  returns a lower score for better performance. For classification, the node addition process tries 
  to minimize the class **impurity** of the set of observations in each of the two branches coming 
  out of the node relative to the impurity of the set of observations entering the node. There are 
  several possible metrics for impurity. They are similar in that they all assign a loss of zero when 
  the the set of observations includes only one class, but differ in how severely the metrics penalize 
  mixed class compositions. One very commonly used metric is the **Gini Index**, 
  `f(p.i) = p.i * (1 - p.i)`, where `p.i` is proportion of observations input to the node that belong 
  to class `i`, and `(1 - p.i)` represents the proportion in other classes. Observations with missing 
  values for the split variable are not counted in the impurity calculation. Node impurity is then
  `sum(f(p.i))`, or the sum of class impurities across classes. For regression, the loss is typically 
  something like the mean-squared-error (MSE), or a robust (less influenced by outliers) alternative. 
  Tree fitting proceeds one node at a time, finding the variable and split value that most minimizes 
  the loss in each of the two branches coming out of the node. The process terminates when some 
  minimal node size is achieved or no further substantial improvement is seen in the loss. This 
  stopping results in a **leaf node** or **terminal node** in the tree, which is where the final 
  response value prediction value is assigned. The predicted value is based on the response values of 
  the training-set observations that end up in that leaf during the fitting/tree-building process. 
  For classification, the most frequently occurring class among these training-set observations is 
  assigned to any new observations that arrive at this node. For regression, the mean of the response 
  in the training-set observations assigned to this node is used as the predicted value for new 
  observations that pass into this node.

In the example below, we with use the `rpart::rpart()` function for fitting. The fitting process
  is controlled by an object generated by the `rpart::rpart.control()` function. This controls
  the complexity of the tree as well as parameter tuning using cross-validation. Such control 
  objects exist for many modeling functions, including `lm()`, `glm()`, and `loess()`. The 
  primary `rpart()` argument to be tuned is `cp`, or the **complexity parameter**. This is a 
  numeric value that specifies by what proportion a new node split must decrease the loss function 
  in order to be considered for addition to the tree. The `rpart()` function can tune `cp` using 
  cross-validation with the fold-number, set by the `xval` argument, defaulting to `10`. Another 
  argument used for limiting tree size (and therefore model complexity) is `min.split`, which is 
  the minimum number of training observations that must be assigned to a node for further splits 
  along that path to be considered. In addition, we can limit the minimum number of training-set 
  observations assigned to a leaf using the `min.bucket` argument. If a split would result in a 
  child node smaller than `min.bucket`, the split will not be added to the tree. We can also limit 
  the maximum tree depth using the `maxdepth` argument, which limits the total number of conditionals 
  that can be applied in series to any single observation.

Below, we treat prostate cancer recurrence as a binary response which we try to predict using 
  variables such as patient age, tumor grading by various schemes and some other cytological measures.

```
library(rpart)
library(caret)

rm(list=ls())

## reformat prostate cancer recurrence dataset; 
##   reformat pgstat to factor indicating whether there was recurrence:

?rpart::stagec
dat <- rpart::stagec
dat$pgstat <- c('no', 'yes')[dat$pgstat + 1]
dat$pgstat <- factor(dat$pgstat)  ## character -> factor
dat$pgtime <- NULL                ## drop column
summary(dat)                      ## note the NAs (missing values)
head(dat)
nrow(dat)

## split into training and test-set in way amenable to CV:
set.seed(1)
idx <- 1:nrow(dat)
folds <- caret::createMultiFolds(idx, k=5, times=12)
idx.trn <- folds[[1]]
dat.trn <- dat[idx.trn, ]
dat.tst <- dat[-idx.trn, ]
summary(dat.trn)
summary(dat.tst)

## take a peak:
par(mfrow=c(1, 1))
plot(dat.trn)

## fit the model, tuning 'cp' complexity parameter by (10-fold) CV:
fit0 <- rpart(pgstat ~ ., data=dat.trn, method='class')
fit0
class(fit0)
is.list(fit0)
names(fit0)
summary(fit0)
plot(fit0)                        ## plot the tree
text(fit0)                        ## add node labels
plotcp(fit0)                      ## plot cross-validation
printcp(fit0)                     ## print cross-validation (xerror is CV error)
(tbl <- fit0$cptable)

## prune tree using 'cp' value chosen from tuning:
(idx.bst <- which.min(tbl[, 'CP']))
tbl[idx.bst, ]
(fit <- prune(fit0, cp=tbl[idx.bst, 'CP']))
plot(fit)
text(fit, cex=0.75)

## make (probabilistic) predictions and take a look:
(prd.tst <- predict(fit, newdata=dat.tst, type='prob'))
prd.tst <- prd.tst[, 'yes']
roc.tst <- pROC::roc(dat.tst$pgstat == 'yes', prd.tst, direction='<')
roc.tst$auc
pROC::ci.auc(roc.tst)
(prd.class.tst <- c('no', 'yes')[(prd.tst > 0.5) + 1])
## mcnemar's test compares sensitivity and specificity:
(cnf.tst <- caret::confusionMatrix(dat.tst$pgstat, factor(prd.class.tst)))

## do predictions work better on training set?
prd.trn <- predict(fit, newdata=dat.trn, type='prob')
prd.trn <- prd.trn[, 'yes']
roc.trn <- pROC::roc(dat.trn$pgstat == 'yes', prd.trn, direction='<')
roc.trn$auc
pROC::ci.auc(roc.trn)
prd.class.trn <- c('no', 'yes')[(prd.trn > 0.5) + 1]
(cnf.trn <- caret::confusionMatrix(dat.trn$pgstat, factor(prd.class.trn)))

```

[Return to index](#index)

---

### Check your understanding 1

Starting with the following:

```
library(rpart)
library(caret)

rm(list=ls())

## reformat prostate cancer recurrence dataset:
dat <- rpart::stagec
dat$pgstat <- c('no', 'yes')[dat$pgstat + 1]
dat$pgstat <- factor(dat$pgstat)  ## character -> factor
dat$pgtime <- NULL                ## drop column

```

Using 5-fold cross-validation repeated 12 times, generate a point estimate of the AUC
  for an `rpart` classification tree with formula `pgstat ~ .` where the `cp` complexity
  parameter is chosen using an inner cross-validation loop. Hint: you don't need to
  do the inner cross-validation explicitly -- `rpart()` does it for you.

[Return to index](#index)

---

### Random forest

A tree model's prediction error is largely attributable to high variance rather than bias
  (assuming the tree is allowed to grow sufficiently large), and the model response variable 
  predictions are non-linear functions of the predictors. The performance of classifiers with 
  these attributes can often be substantially improved through **bagging**. Bagging, or 
  **boostrap aggregation** describes fitting a separate model to a boostrap sample of the 
  original training-set observations. That is, we sample the training-set observations with 
  replacement. We then fit a separate tree model to each of the bootstrap samples. The set of 
  resulting tree models, are used as an **ensemble** or **committee** of models, whose 
  predictions are combined. The final prediction for that new observation is determined by 
  either plurality vote (in the case of classification with a categorical response and discrete 
  class prediction) or by averaging (in the case of regression with a continuous response 
  variable or probabilistic predictions of class membership). The ensemble model's variance is 
  typically expected to be reduced relative to the original model, but the bias is expected to 
  be the same, so the average ensemble model should have higher accuracy than a single tree.

One byproduct of the bagging process, is that for each tree, there will tend to be some 
  observations that do not appear in the bootstrap sample used to train that tree. These 
  observations can serve as independent test data for estimating the performance of that
  individual tree. These performance estimates are referred to as **out-of-bag** or
  **OOB** estimates.

The bagging process does not work very well when the predictions of different classifiers in 
  the ensemble are highly correlated to one another. This is one reason that bagging tends 
  to work better for non-linear classifiers than linear ones: linear classifiers built on 
  different bootstrap samples of the same dataset tend to be more correlated with one another 
  than non-linear classifiers will be. In the case of trees, we can **decorrelate the trees** built 
  using different bootstrap samples by only considering a randomly selected subset of features 
  of size `mtry` for splitting at each node within each tree. This random feature sampling at 
  each node tends to induce major topological differences and differences in variables chosen for 
  each split across the different trees in the ensemble, drastically reducing correlations 
  between trees at the expense of potentially introducing some bias. The trade-off between these 
  two effects is controlled by tuning the number of features `mtry` randomly selected at each 
  node. If there are very few informative features, the optimum `mtry` will tend to be larger 
  so that most node fits are likely get to consider a useful feature and result in a productive 
  split. By contrast, if there are many informative features, smaller `mtry` results in feature 
  sets are still relatively likely to include useful features, so we can take advantage of 
  the better decorrelation offered by smaller feature sets without affecting the performance 
  (biasing) individual trees. The higher the proportion of correlated features in the original
  data, the higher the correlation between predictions from different trees. Under these 
  circumstances, the higher decorrelation offered by smaller feature sets results in better 
  overall performance of the ensemble. In general, wherever practical, we will tune the `mtry` 
  parameter by cross-validation to balance improved tree decorrelation against worsening tree 
  bias. 

**Random forest** is an ensemble modeling approach built on recursive partitioning tree models,
  where **bagging** is used to generate the series of models, and **random feature selection** at 
  each node within each tree is used to decorrelate trees in the ensemble from one another, 
  improving the variance reduction afforded by the bagging. Ensemble methods are often criticized 
  for trading away interpretability for improved predictive performance. The models do tend to be
  very complex, but there are several tools that help us determine both the importance of
  individual features for the performance of the ensemble, as well as the shape of the **marginal 
  relationship** (keeping all other feature values constant) between the response predictions and 
  individual variables of interest. 

**Variable importance** is usually estimated in one of two ways. The first uses the magnitude of 
  reduction in loss (purity or sums-of-squares) resulting from splitting a variable in one tree 
  contributes to the importance of the variable. By summing the importance across all the nodes 
  involving this variable across all trees in the ensemble, we can express an overall importance. 
  Another approach is to use an 'out-of-bag' estimate of the prediction error: for each tree and 
  each variable, we can calculate the OOB error using the original data, then with the variable of 
  interest permuted (so it no longer has a relationship to the response). The change in performance, 
  averaged across all the trees in the forest, is attributed to the permuted variable. Smaller values 
  for `mtry` tend to make variable importances more similar to one another, because it tends to give 
  weaker variables a larger chance of being selected at each split, since stronger variables might
  not be the randomly selected subset.

We can get a feel for the 'shape' of the relationship between an individual feature (say `x.i`) and 
  the response prediction using **partial dependence** plots. To generate such a plot, we take each 
  observation in the training set, set the value of `x.i` to a particular value, while leaving the
  rest of the features unchanged, and generate a response prediction for the modified observation using
  the previously fit random forest model. By doing this for all observations across a sequence of `x.i` 
  values, we can see how changing `x.i` affects predictions when other features remain constant. We 
  use the `randomForest::partialPlot()` function for this purpose.

```
library(caret)
library(pROC)
library(glmnet)
library(randomForest)

## data:

rm(list=ls())
data(dhfr)                        ## from caret
dat <- dhfr
class(dat)
dim(dat)
table(dat$Y)
class(dat$Y)

## generate hold-out test-set:

set.seed(1)
idx <- 1 : nrow(dat)
folds <- caret::createMultiFolds(idx, k=5, times=3)
idx.trn <- folds[[1]]

dat.trn <- dat[idx.trn, ]
dat.tst <- dat[-idx.trn, ]
names(dat.trn)[1]

## mtry tuning using out-of-bag error:

(tune.fit <- tuneRF(x=dat.trn[, -1], y=dat.trn$Y, stepFactor=0.5, improve=0.01, 
  ntreeTry=1000, trace=T, plot=T, doBest=F, replace=T))
class(tune.fit)
(score.best <- min(tune.fit[, 'OOBError']))
(i.best <- tune.fit[, 'OOBError'] == score.best)
(mtry.best <- min(tune.fit[i.best, 'mtry']))

## fit with selected mtry:

fit <- randomForest(x=dat.trn[, -1], y=dat.trn$Y, mtry=mtry.best, ntree=1000, importance=T, replace=T)
par(mfrow=c(1, 1))
plot(fit, log='y')

## probabilistic predictions:

prd.tst <- predict(fit, newdata=dat.tst[, -1], type='prob')
prd.trn <- predict(fit, newdata=dat.trn[, -1], type='prob')

head(prd.tst)
prd.tst <- prd.tst[, 'active']
prd.trn <- prd.trn[, 'active']

## evaluation: 

roc.tst <- pROC::roc(dat.tst$Y == 'active', prd.tst, direction='<')
roc.tst$auc
pROC::ci.auc(roc.tst)
prd.class.tst <- c('inactive', 'active')[(prd.tst > 0.5) + 1]
(cnf.tst <- caret::confusionMatrix(dat.tst$Y, factor(prd.class.tst)))

roc.trn <- pROC::roc(dat.trn$Y == 'active', prd.trn, direction='<')
roc.trn$auc
pROC::ci.auc(roc.trn)
prd.class.trn <- c('inactive', 'active')[(prd.trn > 0.5) + 1]
(cnf.trn <- caret::confusionMatrix(dat.trn$Y, factor(prd.class.trn)))

## importance: type=2 is average (over trees) decrease in node impurity achieved by splitting on variable

imp.trn <- randomForest::importance(fit, type=2)
imp.trn[1:30, , drop=F]
imp.trn <- imp.trn[order(imp.trn, decreasing=T), , drop=F]
imp.trn[1:5, ]
par(mfrow=c(1, 1))
randomForest::varImpPlot(fit, type=2, mex=0.75, cex=0.75)

## partial dependence plot:

rownames(imp.trn)[1:6]
par(mfrow=c(2, 3))
for(idx in 1:6) {
  (x.var1 <- rownames(imp.trn)[idx])
  randomForest::partialPlot(fit, pred.data=dat.trn, x.var=c(x.var1), which.class='active', main=x.var1)
}

```

[Return to index](#index)

---

### Check your understanding 2

Use the `dhfr` data from the `caret` package to perform 5-fold cross-validation repeated twice to 
  estimate the AUC for a model with `Y` as categorical response and the rest of the features as
  predictors. Tune the `mtry` parameter using the `tuneRF()` function, specifying `stepFactor=0.5` 
  and `improve=0.01`.

[Return to index](#index)

---

### Boosting

Boosting is another ensemble method that can serve to increase performance by combining the output 
  of many **weak learners** (models that don't perform very well) into a final model. The method 
  is most often mentioned in the context of trees, but can be used with other weak learners as well. 
  Trees are a popular choice, since they can readily represent non-linear relationships and complex
  interactions in the training data. By contrast, linear models impose more assumptions about the 
  form of the relationship between the response and predictors. The trees used for boosting tend 
  to be very simple single node trees, sometimes referred to as **decision stumps**. Because a 
  single node tree imposes a very simplistic structure on the relationship between response and 
  predictors (only a single predictor with a single cut point is included), they tend to 
  systematically oversimplify the relationship. Therefore, they are relatively **biased** models, 
  in addition to having the high variance seen with even more complex decision trees. Like 
  bagging, boosting tends to decrease model variance, but the biggest payoff seen with boosting 
  is often the reduction in model bias. **Gradient boosting** (a particular type of boosting) 
  using decision stumps has proven itself a very competitive algorithm across a wide variety of 
  datasets in machine learning competitions, and has often been used to win such competitions. The 
  performance of gradient boosted trees tends to be similar to or slightly better to random forest. 
  However, tuning a boosted tree model is more complicated and can require a great deal more 
  computation than tuning a random forest model.

Boosting starts with an initial model fit to the training-set. The training-set observations are 
  then weighted based on how well they fit the initial model. Observations whose response values
  are poorly predicted (have larger residuals) are weighted more than observations which the model
  predicts better. A second model is fitted to the weighted observations. Due to the weighting,
  this model will tend to focus on better prediction of the observations poorly predicted by the
  previous model. The observation weights are then adjusted to reflect the accuracy with which the 
  current model predicts the training-set observation response values. Then a third model is fit to 
  the re-weighted observations, and the entire process is repeated until a set iteration count 
  limit is reached or a convergence criterion is met. Limiting the number of iterations limits 
  the potential for overfitting the training-set, but performing too few iterations restricts the 
  ability of the model to capture the true underlying relationship between response and predictors.

The main differences between modern boosting algorithms is in how the observation weights are updated
  at each iteration of the algorithm. The simpler to understand **AdaBoost.M1** algorithm is described
  below as it could be used for classification, where we assume for simplicity that the observed 
  response `y` and model prediction `f(x)` are both categorical (one alternative would be `f(x)` 
  yielding a continuous class probability):

1) Set weights for `n` observations in training-set equal: `wts.nxt <- 1/n`.
2) Repeat `M` times; for iteration `m`:
    * set current observation weights `wts.m <- wts.nxt`
    * fit stump to weighted (by `wts.m`) observations yielding model predictions `f.m(x)`
    * compute model error `err.m` as weighted (by `wts.m`) average of `n` observation errors.
    * compute **step-size** for weight adjustment: `adj.m <- log((1 - err.m)/err.m)`. 
    * update observation weights: `wts.nxt <- wts.m * exp(adj.m * as.numeric(y == f.m(x))`
3) Final model output is `sum(adj.m * f.m(x))`, where the sum is over all M models.

The effect of reweighting observations with observation weights determined by residual sizes from
  the current model is similar in effect to fitting the later model to the residuals of the 
  previous model. This means that the process is sensitive to inclusion of outliers and also that
  enough iterations will eventually lead to fitting all the noise in the training-set, which can
  lead to poor performance on new data.

A more modern boosting algorithm that tends to perform somewhat better is **gradient boosting**.
  The gradient boosting algorithm adjusts observation weights in a more complex but effective way.
  Treating each observation weight as a different dimension in a multi-dimensional space (where
  the are `n` dimensions, one for each observation), gradient boosting finds the direction in 
  this space that most quickly decreases the loss function (estimated using the training-set).
  For instance, for a continuous response and mean-squared error (MSE) as the loss function, we 
  look for the direction in the observation weight space in which a fixed step-size results in 
  the largest reduction in MSE. We then adjust the observation weights by taking that step in 
  the weight space. The size of the step is determined by a **learning rate** parameter. Setting 
  the learning-rate to a value of less than one shrinks the step size, resulting in some  
  regularization that reduces chance of overfitting, but also increases number of steps needed 
  to converge upon the true relationship between response and predictors. Fitting requires
  balancing the learning-rate against the number of iterations. One good general approach is to 
  pick a learning-rate that is as small as tolerable given available time and compute resource, 
  then tune the number of iterations by cross-validation. Other options include tuning the 
  size of the tree using a variety of options (like the maximum height and the minimum leaf
  size, which are also tuning parameters for `rpart()` and `randomForest()`). Most implementations
  of gradient boosting also offer the option of sampling observations with replacement (bagging)
  at each step in the process in order to reduce the chance of overfitting the training-set, which
  reduces model variance.

```
library(caret)
library(pROC)
library(gbm)

## data:

data(dhfr)                        ## from caret
dat <- dhfr
class(dat)
dim(dat)
table(dat$Y)
class(dat$Y)                      ## need this to be 0/1

dat$Y <- as.numeric(dat$Y == 'active')
table(dat$Y)
class(dat$Y)

## generate hold-out test-set:

set.seed(1)
idx <- 1 : nrow(dat)
folds <- caret::createMultiFolds(idx, k=5, times=3)
idx.trn <- folds[[1]]

dat.trn <- dat[idx.trn, ]
dat.tst <- dat[-idx.trn, ]

fit <- gbm::gbm(Y ~ ., data=dat.trn, distribution="bernoulli", 
  n.trees=1000, shrinkage=0.01, interaction.depth=3, n.minobsinnode=10, 
  cv.folds=5, keep.data=F, verbose=T, n.cores=1)

fit
class(fit)
is.list(fit)
names(fit)

## best stopping point; in plot, training error black, cv error green:

(n.trees.best <- gbm.perf(fit, method="cv"))
summary(fit, n.trees=1)                ## first tree
summary(fit, n.trees=n.trees.best)     ## final series of trees

## probabilistic predictions:

(prd.tst <- predict(fit, newdata=dat.tst, n.trees=n.trees.best, type="response"))
prd.trn <- predict(fit, newdata=dat.trn, n.trees=n.trees.best, type="response")

## evaluate:

roc.tst <- pROC::roc(dat.tst$Y, prd.tst, direction='<')
roc.tst$auc
pROC::ci.auc(roc.tst)
prd.class.tst <- (prd.tst > 0.5) + 1
(cnf.tst <- caret::confusionMatrix(factor(dat.tst$Y + 1), factor(prd.class.tst)))

roc.trn <- pROC::roc(dat.trn$Y, prd.trn, direction='<')
roc.trn$auc
pROC::ci.auc(roc.trn)
prd.class.trn <- (prd.trn > 0.5) + 1
(cnf.trn <- caret::confusionMatrix(factor(dat.trn$Y + 1), factor(prd.class.trn)))

## importance plots, can be multivariate; can specify variables by integer index or name:

smry <- summary(fit, n.trees=n.trees.best)
smry$var[1:6]

plot(fit, i.var=smry$var[1], n.trees=n.trees.best)  
plot(fit, i.var=smry$var[2], n.trees=n.trees.best)  
plot(fit, i.var=smry$var[3], n.trees=n.trees.best)  
plot(fit, i.var=smry$var[4], n.trees=n.trees.best)  

```

[Return to index](#index)

---

### Check your understanding 3

Starting with the following data:

```
library(rpart)

rm(list=ls())

## reformat prostate cancer recurrence dataset:
dat <- rpart::stagec
dat$pgtime <- NULL                ## drop column

```

Use 5-fold cross-validation, repeated three times to estimate the AUC of a gradient boosted 
  tree model, built using `gbm()` with `shrinkage=0.01`, `interaction.depth=2`, and 
  `n.minobsinnode=5`. Use an inner 5-fold cross-validation loop to select the number of
  iterations. Hint: the `gbm()` function does the inner-loop parameter tuning for you.

[Return to index](#index)

---

## FIN!
