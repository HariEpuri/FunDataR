###################################################################################################
L1C1:

1) Draw 1000 samples from a uniform distribution on the interval [-5, 5] and store them in the variable x. Make a histogram of x.

rm(list=ls())
set.seed(1)
x <- runif(1000, min=-5, max=5)
hist(x)

2) Draw 1000 samples from a normal distribution with mean 0 and standard deviation 2 and store the samples in the variable y. Make a histogram of y. Try to fiddle with the number of histograms 'bins' (bins parameter to hist()). Trying values in c(3, 10, 30, 100, 300) should provide a 'feel' for what happens when too many or too few.

y <- rnorm(1000, mean=0, sd=2)
par(mfrow=c(2, 3))
length(hist(y)$breaks)
hist(y, breaks=3)
hist(y, breaks=10)
hist(y, breaks=30)
hist(y, breaks=100)
hist(y, breaks=300)
par(mfrow=c(1, 1))

3) Concatenate x and y into a single vector of length 2000. Generate a 2D plot of the result. From the plot, can you tell where one distribution (say the one from x) stops and the other (from y) starts?

z <- c(x, y)
plot(z)

4) Add a horizontal line to the plot from question #3, at the mean of z. Hint: abline parameter h for 'horizontal'.

abline(h=mean(z), lty=2, col='magenta')

5) Add a vertical line to the plot at the 1000th position in the plot from question #4. Hint: abline parameter v for 'vertical'; note the bottom axis positions are the order (or index positions) of the values in z, so the 1000th position marks the boundary between the values from x (uniform) and those from y (normal). Are some differences in the distribution of points between the left side and right side of your horizontal line apparent to you?

abline(v=1000, lty=3, col='cyan')

###################################################################################################
L1C2:

1) Generate a loop that draws 10000 samples of size 10 from a uniform distribution on the interval [-1, 1].

rm(list=ls())
set.seed(1)

for(i in 1 : 10000) {
  smp <- runif(10, min=-1, max=1)
}

2) Add to your loop in order to create a vector (length == 10000) of the standard deviations for each sample.

rm(list=ls())
set.seed(1)

rslt <- numeric(length=0)

for(i in 1 : 10000) {
  smp <- runif(10, min=-1, max=1)
  rslt <- c(rslt, sd(smp))
}

3) Make a histogram of the distribution of standard deviations.

hist(rslt)
sd(runif(1e6, -1, 1))

4) Repeat this process for a uniform distribution on the interval [-100, 100].

rm(list=ls())
set.seed(1)

rslt <- numeric(length=0)

for(i in 1 : 10000) {
  smp <- runif(10, min=-100, max=100)
  rslt <- c(rslt, sd(smp))
}
hist(rslt)
sd(runif(1e6, -100, 100))


###################################################################################################
L2C2:


###################################################################################################
L3C2:

Using the mtcars built-in dataset:

    Conduct an ANOVA omnibus F-test on the null hypothesis that the mean gas efficiency (mtcars$mpg) for cars with different numbers of cylinders (mtcars$cyl) are all equal. What is the p-value?

    Conduct the TukeyHSD to look for pairwise differences in group means. What are the p-values for individual comparisons?

    Do a qqnorm() and qqline() plot of the residuals from the model.

    Perform a Shapiro test for normality of residuals.

    Perform a Bartlett test for equal variances within groups.


rm(list=ls())
dat <- mtcars
summary(dat)
## fit <- aov(mpg ~ factor(cyl), data=dat)
fit <- aov(mpg ~ as.character(cyl), data=dat)
coef(fit)
summary(fit)
TukeyHSD(fit)

qqnorm(residuals(fit))
qqline(residuals(fit))

shapiro.test(residuals(fit))
bartlett.test(mpg ~ factor(cyl), data=dat)


###################################################################################################
L4C1:

## 1: Plot mpg vs. disp using the formula syntax.

rm(list=ls())
dat <- mtcars
plot(mpg ~ disp, data=dat)

## 2: Fit a linear model with mpg as the response and disp as the explanatory variable. Extract the 
##    intercept estimate. Extract the slope estimate. Get the fitted values. Get the residuals.

fit <- lm(mpg ~ disp, data=dat)
coef(fit)['(Intercept)']
coef(fit)['disp']
fitted(fit)
residuals(fit)

## 3: Summarize the fit. Extract the p-value for the null hypothesis that the regression line passes 
##    through the origin (or y==0 when x==0). Extract the p-value for the null hypothesis that the 
##    conditional mean of y does not depend on x. Hint: think about the meaning of the coefficients 
##    returned.

(smry <- summary(fit))
coef(smry)['(Intercept)', 'Pr(>|t|)']
coef(smry)['disp', 'Pr(>|t|)']

###################################################################################################
L4C2:

## Generate a linear fit of the mtcars data with mpg as response variable and disp as a continuous 
##   explanatory variable. Generate a single figure area containing plots 1 thru 6 from plot.lm() 
##   called on your model.

fit <- lm(mpg ~ disp, data=mtcars)
par(mfrow=c(2, 3))
plot(fit, which=1:6)
par(mfrow=c(1, 1))

## 1: Does the 'residuals vs. fitted' plot suggest the relationship is truly linear?

# the plot shows a v-shaped trend, which suggests perhaps a more flexible fit should be considered.
#   on the other hand, the 'trend' may be due to outliers in the upper right of the plot.

## 2: What does the 'scale vs. location' plot suggest about the relationship between residual 
##    variance and larger predicted values? Is that consistent with assumptions behind the p-values 
##    summary() will generate for your coefficients?

# it suggests that residual size (scale) tends to increase with larger values (location) of the 
#   response variable. 

## 3: Are there any 'highly influential' data points?

# none of the Cook's distances are greater than 0.5, so no observations are particularly influential.

## 4: Does the 'residuals vs. leverage' plot suggest that 'Toyota Corolla' is influential more 
##    because of how far the mpg value is from the modeled conditional mean or how far the disp 
##    value is from the sample average?

# average leverage is expected to be 0.0625:
length(coef(fit)) / nrow(mtcars)

# toyota corolla leverage looks like about 0.08, so only slightly higher than average;
#   scale-location and standardized residuals-vs-leverage plots both show toyota corolla residual 
#   is larger than the rest. So the mpg value seems more 'out-of-line' with the rest of the values 
#   than does the disp value for this observation.

###################################################################################################
L4C3:

## Using the built-in data set 'faithful':

## 1: Assign the sample observations randomly so about 80% are in a training-set and 
##    the remaining 20% in a test-set.

rm(list=ls())
dat <- faithful
n.trn <- round(nrow(dat) / 5)

idx.trn <- sample(1 : nrow(dat), n.trn, replace=F)
i.trn <- rep(F, nrow(dat))
i.trn[idx.trn] <- T
i.tst <- ! i.trn

dat.trn <- dat[i.trn, ]
dat.tst <- dat[i.tst, ]

## 2: Fit a simple linear regression model to the training-set observations designating
##    `waiting` as the response and `eruptions` as the predictor. 

fit <- lm(waiting ~ eruptions, data=dat.trn)

## 3: Make predictions for both the training-set and test-set.

pred.trn <- fitted(fit)
## pred.trn <- predict(fit, newdata=dat.trn)
pred.tst <- predict(fit, newdata=dat.tst)

## 4: Use the f.mse() function above to estimate model performance on both the training and test-sets.
##    What sort of performance do you see if you use the global mean to predict values for the test-set.

f.mse <- function(y, y.hat) {

  if(! (is.numeric(y) && is.numeric(y.hat)) )
    stop("y and y.hat must be numeric")

  if(length(y) != length(y.hat))
    stop("y and y.hat must be same length")

  if(length(y) == 0) return(NaN)

  mean((y - y.hat) ^ 2)
}

f.mse(dat.trn$waiting, pred.trn)
f.mse(dat.tst$waiting, pred.tst)
f.mse(dat.tst$waiting, rep(mean(dat.trn$waiting), nrow(dat.tst)))


