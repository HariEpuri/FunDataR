Multivariate statistics

###################################################################################################
## Lesson 1 : Check 1 

## 1) Fit a linear model using `lm()` with `trees$Volume` as response and
##    `trees$Height` using the 'optimal' `lambda` value suggested above (instead
##    of rounding `lambda` to `0` and using the `log()`) to transform the 
##    response. How do the residual plots differ from using the `log()`?

rm(list=ls())

fit1 <- lm(Volume ~ Height, data=trees)
out <- boxcox(fit1)
(lambda.best <- out$x[which.max(out$y)])

fit2 <- lm(log(Volume) ~ Height, data=trees)

dat <- trees
dat$Volume <- dat$Volume ^ lambda.best
fit3 <- lm(Volume ~ Height, data=dat)

par(mfrow=c(2, 3))
plot(fit2, which=1:6)             ## homoskedastic, but not very normal

dev.new()                         ## open new plotting window
par(mfrow=c(2, 3)) 
plot(fit3, which=1:6)             ## pretty similar to fit2, but hard to interpret

dev.off()                         ## close extra plotting window


## 2) Fit another linear model with `lm()` with `trees$Volume` as response and
##    `trees$Height` trying a `lambda` value of `-1` (corresponding to the 
##    reciprocal), to transform the response. How do the residual plots 
##    differ from using the `log()`?

rm(list=ls())

fit1 <- lm(log(Volume) ~ Height, data=trees)

dat <- trees
dat$Volume <- dat$Volume ^ -1
fit2 <- lm(Volume ~ Height, data=dat)

par(mfrow=c(2, 3))
plot(fit1, which=1:6)             ## homoskedastic, but not very normal

dev.new()                         ## open new plotting window
par(mfrow=c(2, 3)) 
plot(fit2, which=1:6)             ## more normal, but maybe heteroskedastic; outlier influence larger

dev.off()                         ## close extra plotting window


###################################################################################################
## Lesson 1 : Check 2 

## 1. Repeat the simulation of position vs time of an accelerating system from above (copy and
##    paste). Then split the data into 20% test set and 80% training set (copy and paste). 
##    Then fit a model of `y ~ tm` to the training data. Then fit another model of `sqrt(y) ~ tm` to 
##    the training data. Finally, fit a model of `y ~ I(tm ^ 2)`.

rm(list=ls())
set.seed(1)

# simulate data:
n <- 100                          ## sample size
p.tst <- 0.2                      ## proportion for test set
n.tst <- round(p.tst * n)         ## test set size
accel <- 3                        ## acceleration
y.init <- 1000                    ## initial position
tm <- runif(n, min=0, max=n)      ## time
y.mdl <- y.init + 0.5 * accel * (tm ^ 2)
err <- rnorm(length(tm), mean=0, sd=150)
dat <- data.frame(y=y.mdl + err, tm=tm)

# split into training and test sets:
idx.tst <- sample(1 : n, n.tst, replace=F)
i.tst <- rep(F, n)
i.tst[idx.tst] <- T
dat.tst <- dat[i.tst, ]
dat.trn <- dat[! i.tst, ]

# fit the models:
fit0 <- lm(y ~ tm, data=dat.trn)
fit1 <- lm(sqrt(y) ~ tm, data=dat.trn)
fit2 <- lm(y ~ I(tm ^ 2), data=dat.trn)

## 2. Generate residual plots for all three fits and rank the fits with regard to how well
##    you feel they meed assumptions for the linear model.

# plot the residuals:
par(mfrow=c(2, 3))
plot(fit0, which=1:6)             ## pretty bad
plot(fit1, which=1:6)             ## better, but not so great
plot(fit2, which=1:6)             ## much better

## 3. Make predictions of the response values for the observations in the test set. Generate
##    estimates of the mean-squared-error for each fit based on the test set.

# predictions for fits: 
pred0 <- predict(fit0, newdata=dat.tst)
pred1 <- predict(fit1, newdata=dat.tst)
pred1 <- pred1 ^ 2                ## MUST UNDO TRANSFORMATION!!!
pred2 <- predict(fit2, newdata=dat.tst)

# prediction error:
mean((dat.tst$y - pred0) ^ 2)
mean((dat.tst$y - pred1) ^ 2)
mean((dat.tst$y - pred2) ^ 2)

## 4. Plot the predicted response values on the y/left/vertical axis and observed response values on the
##    x/bottom/horizontal axis for all three fits.

par(mfrow=c(1, 3))

plot(x=dat.tst$y, y=pred0, main="y ~ tm")
abline(a=0, b=1)

plot(x=dat.tst$y, y=pred1, main="sqrt(y) ~ tm")
abline(a=0, b=1)

plot(x=dat.tst$y, y=pred2, main="y ~ I(tm ^ 2)")
abline(a=0, b=1)


###################################################################################################
## Lesson 1 : Check 3 

## 1. Split the built-in 'DNase' dataset into a test-set consisting of `Run == 3` and `Run == 6` and a 
##    training-set consisting of the other nine `Run` values.

i.tst <- DNase$Run %in% c(3, 6)
dat.tst <- DNase[i.tst, ]
dat.trn <- DNase[! i.tst, ]
summary(dat.tst$Run)              ## looks right
summary(dat.trn$Run)              ## looks right

## 2. Fit a linear model of `density ~ conc` to the training data. Make predictions on the held-out test-set.

fit.lm <- lm(density ~ conc, data=dat.trn)
pred.lm <- predict(fit.lm, newdata=dat.tst)

## 3. Fit a loess model to the same formula, using the default parameter settings. Make predictions on the
##    held-out test-set.

fit.loess <- loess(density ~ conc, data=dat.trn)
pred.loess <- predict(fit.loess, newdata=dat.tst)

## 4. Calculate the mean-squared error for the two models based on the test-set predictions.

mean((dat.tst$density - pred.lm) ^ 2)
mean((dat.tst$density - pred.loess) ^ 2)


###################################################################################################
## Lesson 2 : Check 1

1. From the built-in `warpbreaks` dataset, split out the `breaks` values where `wool == 'A'` 
   and `tension == 'L'` as one variable (say `x`). Split out the `breaks` values where 
   `wool == 'B'` and `tension == 'M'` as another variable (say `y`). 

rm(list=ls())

dat <- warpbreaks
i.x <- dat[, 'wool'] == 'A' & dat[, 'tension'] == 'L'
i.y <- dat[, 'wool'] == 'B' & dat[, 'tension'] == 'M'

x <- dat[i.x, 'breaks']
y <- dat[i.y, 'breaks']
table(dat[, 'wool'], dat[, 'tension'])
length(x)
length(y)

2. Rearrange the data to be in a data-frame with `length(x) + length(y)` rows and two columns. The 
   first column should be the `breaks` and the second column should be a new indicator, of type
   `character` or `factor`, representing group membership. The first group (maybe labeled 'A') is
   the data with `wool == 'A'` and `tension == 'L'`, and the other (maybe 'B') is the data with
   `wool == 'B'` and `tension == 'M'`. Hint: concatenate `x` and `y` to make the first column; 
   use their lengths and `rep()` to specify the group variable.

grps = c(rep('A', length(x)), rep('B', length(y)))
(dat <- data.frame(x=c(x, y), grp=grps))

3. Conduct a Bartlett test to see if the variances between the two groups is different.

bartlett.test(x=c(x, y), g=grps)

4. We will assume the Bartlett test showed no evidence of unequal variances, so conduct a 
   two-sample equal variances t-test on the null hypothesis that the mean `breaks` of 
   group `A` and group `B` (or equivalently, `x` and `y`) are different. Return the 
   statistic (t-statistic in this case) and p-value from the test.

fit <- t.test(x=x, y=y, var.equal=T)
fit$statistic
fit$p.value

5. Conduct 9999 permutations of group labels, conducting the same `t.test()` described 
   above, saving the statistic for each iteration. Compare the distribution of permutation
   results to the original unpermuted result in order to calculate a permutation p-value. 
   Does it corroborate or contradict the parametric test? Hint: this will likely be 
   easier if you use the data.frame you created above and permute the group labels.

set.seed(1)                       ## make pseudo-random process reproducible
R <- 9999                         ## number of permutations
rslts <- rep(as.numeric(NA), R)   ## to hold results; pre-extended for efficiency

for(i in 1 : R) {
  dat.i <- dat
  dat.i$grp <- sample(dat.i$grp, nrow(dat.i), replace=F)
  x.i <- dat.i$x[dat.i$grp == 'A']
  y.i <- dat.i$x[dat.i$grp == 'B']
  fit.i <- t.test(x=x.i, y=y.i, var.equal=T)
  rslts[i] <- fit.i$statistic
}

fit$statistic                     ## test statistic from non-permuted (original) data
summary(rslts)                    ## should be no NAs: if there are, need to adjust for them
(n.exceed <- sum(abs(rslts) >= abs(fit$statistic)))
(n.exceed + 1) / (R + 1)          ## permutation p-value (should never be 0!!!)
fit$p.value


###################################################################################################
## Lesson 2 : Check 2

1. Fit a linear model with the formula `sqrt(Volume) ~ Girth` to the built-in trees dataset.

rm(list=ls())
fit <- lm(sqrt(Volume) ~ Girth, data=trees)
summary(fit)
par(mfrow=c(2, 3))
plot(fit, which=1:6)

2. Generate a parametric 95% confidence interval for the for the Girth coefficient.

confint(fit)['Girth', ]           ## the parametric confidence interval

3. Generate a bootstrap 95% confidence interval (BCa if it works, otherwise Percentile) on the
   Girth coefficient. Does it generally corroborate or contradict the parametric interval?

library(boot)
set.seed(1)

## boot() needs function taking data and integer index, returning estimate:

f <- function(dat, i) {
  fit.i <- lm(sqrt(Volume) ~ Girth, data=dat[i, ])
  coef(fit.i)['Girth']
}

f(trees, rep(T, nrow(trees)))     ## check: should return the same estimate as the original fit

R <- 999                          ## number of bootstrap iterations
out <- boot(trees, f, R)          ## the bootstrapping
par(mfrow=c(1, 1))
plot(out)                         ## bias? skew? normal? 
jack.after.boot(out)              ## outliers affect point estimate or CIs?
(ci <- boot.ci(out))              ## bootstrap confidence intervals; BCa preferred
confint(fit)['Girth', ]           ## the parametric confidence interval

###################################################################################################
## Lesson 2 : Check 3

1. Fit a linear model to the formula `Volume ~ Girth` for the built-in `trees` dataset. Fit a 
   second linear model to the formula `sqrt(Volume) ~ Girth`. Generate summaries for each fit.

rm(list=ls())

fit1 <- lm(Volume ~ Girth, data=trees)
fit2 <- lm(sqrt(Volume) ~ Girth, data=trees)
summary(fit1)
summary(fit2)

2) Use 5-fold cross-validation with 3 repetitions (use the `caret::createMultiFolds()` parameter 
   `times` to set the repetitions) to compare the two formulas above in terms of mean-squared 
   error of the resulting models. Examine the mean and standard deviations of the results 
   for each formula to decide if the `sqrt()` transformation is worthwhile. 

library('caret')
set.seed(1)

k <- 5
times <- 3
dat <- trees
frm1 <- Volume ~ Girth
frm2 <- sqrt(Volume) ~ Girth

f <- function(idx) {

  ## split into training and testing:
  dat.trn <- dat[idx, ]
  dat.tst <- dat[-idx, ]

  ## fit Volume ~ Girth:
  fit1 <- lm(frm1, data=dat.trn)
  pred1 <- predict(fit1, newdata=dat.tst)

  ## fit sqrt(Volume) ~ Girth:
  fit2 <- lm(frm2, data=dat.trn)
  pred2 <- predict(fit2, newdata=dat.tst)
  pred2 <- pred2 ^ 2

  ## estimate error for each model:
  mse1 <- mean((dat.tst$Volume - pred1) ^ 2, na.rm=T)
  mse2 <- mean((dat.tst$Volume - pred2) ^ 2, na.rm=T)

  ## return error estimates:
  c(mse.lm=mse1, mse.loess=mse2)
}

idx <- 1 : nrow(dat)
folds <- createMultiFolds(idx, k=k, times=times)

rslt <- sapply(folds, f)
apply(rslt, 1, mean)
apply(rslt, 1, sd)

###################################################################################################
## Lesson 3 : Check 1

## 1. Use 10-fold cross-validation repeated 7 times to see which of the following formula best fit
##   the wtloss training set: `Weight ~ Days`, `Weight ~ I(Days ^ 2)`, or 
##   `Weight ~ Days + I(Days ^ 2)`? Examine both the mean prediction error as well as its spread.

library('caret')
library('MASS')
rm(list=ls())
set.seed(1)

dat <- wtloss

f <- function(idx.trn) {
  dat.trn <- wtloss[idx.trn, ]
  dat.tst <- wtloss[-idx.trn, ]
  fit1 <- lm(Weight ~ Days, data=dat.trn)
  fit2 <- lm(Weight ~ I(Days ^ 2), data=dat.trn)
  fit3 <- lm(Weight ~ Days + I(Days ^ 2), data=dat.trn)
  prd1 <- predict(fit1, newdata=dat.tst)
  prd2 <- predict(fit2, newdata=dat.tst)
  prd3 <- predict(fit3, newdata=dat.tst)
  mse1 <- mean((dat.tst$Weight - prd1) ^ 2, na.rm=T)
  mse2 <- mean((dat.tst$Weight - prd2) ^ 2, na.rm=T)
  mse3 <- mean((dat.tst$Weight - prd3) ^ 2, na.rm=T)
  c(mse1=mse1, mse2=mse2, mse3=mse3)
}

k <- 10                           ## number of folds
times <- 7                        ## number of repetitions
idx <- 1:nrow(wtloss)             ## vector of index positions of observations
folds <- createMultiFolds(idx, k=k, times=times)
rslt <- sapply(folds, f)          ## folds is list of indices to training-set observations

> apply(rslt, 1, mean)            ## average mse over folds
      mse1       mse2       mse3 
13.7152582 88.7467032  0.9426234 

> apply(rslt, 1, sd)              ## spread of results; stability of performance
      mse1       mse2       mse3 
 5.7907744 36.2949816  0.6028187 


## 2. How does the improvement in performance offered by the best fit compare to the standard 
##      deviation of the performance of the best fit?

> (13.7152582 - 0.9426234)  / 0.6028187
[1] 21.18819


###################################################################################################
## Lesson 3 : Check 2

## 1. Peform the following two fits 1000 times and return the standard deviation of the 
##      estimates of the coefficient for `x1` separately for `fit1` and `fit2`:

rm(list=ls())
set.seed(1)

R <- 1000

rslt1 <- rep(as.numeric(NA), R)
rslt2 <- rep(as.numeric(NA), R)

for(i in 1 : R) {
  e <- rnorm(100, mean=0, sd=0.1)   ## error
  x1 <- runif(100, min=0, max=1)    ## predictor 1
  x2 <- runif(100, min=0, max=1)    ## uncorrelated second predictor

  ## correlated third predictor:
  x3 <- 0.999 * x1 + 0.001 * runif(100, min=0, max=1)

  ## rescale to ensure w/i interval [0, 1]:
  x3 <- (x3 - min(x3)) / max(x3)

  y1 <- x1 + x2 + e
  y2 <- x1 + x3 + e
  fit1 <- lm(y1 ~ x1 + x2)
  fit2 <- lm(y2 ~ x1 + x3)

  rslt1[i] <- coef(fit1)['x1']
  rslt2[i] <- coef(fit2)['x1']
}

> sd(rslt1)
[1] 0.03545117

> sd(rslt2)
[1] 33.99997


###################################################################################################
## Lesson 3 : Check 3

## Using the following dataset:

## 
## dat <- mtcars[, c('mpg', 'wt', 'gear')]
## table(dat$gear)
## dat$gear <- factor(dat$gear, ordered=F)
## summary(dat)

## 1) Plot the variables against one another.

rm(list=ls())

dat <- mtcars[, c('mpg', 'wt', 'gear')]
table(dat$gear)
dat$gear <- factor(dat$gear, ordered=F)
summary(dat)
par(mfrow=c(1, 1))
plot(dat)


## 2) Fit the linear model with formula `mpg ~ wt * gear` to the data. Are the individual variable
##      coefficients significantly different from zero? Are the interaction coefficients significantly
##      different from zero?

fit1 <- lm(mpg ~ wt * gear, data=dat)
coef(summary(fit1))


## 3) Plot separate regression lines for three values of `gear`.

wt <- seq(from=min(dat$wt), to=max(dat$wt), length.out=10000)
table(dat$gear)

pred1 <- predict(fit1, newdata=data.frame(wt=wt, gear='3'))
pred2 <- predict(fit1, newdata=data.frame(wt=wt, gear='4'))
pred3 <- predict(fit1, newdata=data.frame(wt=wt, gear='5'))

par(mfrow=c(1, 1))
plot(x=range(dat$wt), y=range(c(pred1, pred2, pred3)), xlab='wt', ylab='mpg', type='n')
lines(x=wt, y=pred1, lty=2, col='cyan')
lines(x=wt, y=pred2, lty=3, col='magenta')
lines(x=wt, y=pred3, lty=4, col='orangered')

i3 <- dat$gear == '3'
i4 <- dat$gear == '4'
i5 <- dat$gear == '5'

points(x=dat$wt[i3], y=dat$mpg[i3], pch='+', col='cyan')
points(x=dat$wt[i4], y=dat$mpg[i4], pch='x', col='magenta')
points(x=dat$wt[i5], y=dat$mpg[i5], pch='o', col='orangered')

legend(
  'bottomleft',
  legend=c('gear3', 'gear4', 'gear5'),
  col=c('cyan', 'magenta', 'orangered'),
  lty=c(2, 3, 4),
  pch=c('+', 'x', 'o')
)

###################################################################################################
## Lesson 4 : Check 1

1) Make a copy of the mtcars dataset and make the 'gears' variable an unordered factor. Fit
   the linear model formula 'mpg ~ wt * gear' to the data. 

rm(list=ls())
dat <- mtcars
dat$gear <- factor(dat$gear, ordered=F)
fit <- lm(mpg ~ wt * gear, data=dat)

2) Extract the coefficient p-values, not including the intercept from the model you just fit. 

smry <- summary(fit)
(coefs <- coef(smry))
(p.values <- coefs[-1, 'Pr(>|t|)'])

3) Adjust the coefficient p-values you extracted using the 'Holm-Bonferroni' FWER procedure, the 
   'Benjamini-Hochberg' FDR procedure, and with the 'Benjamini-Yekutieli' FDR procedure. Note how 
   the results compare in this case, when all the p-values are significant.

p.adjust(p.values, method='holm')
p.adjust(p.values, method='BH')
p.adjust(p.values, method='BY')


###################################################################################################
## Lesson 4 : Check 2


Starting with the following synthetic dataset:

```
rm(list=ls())
set.seed(1)

x1 <- rnorm(5, 1, 1)
x2 <- rnorm(5, 2, 1)
x3 <- rnorm(5, 3, 1)
x4 <- rnorm(5, 4, 1)
e <- rnorm(5, 0, 1)
y <- 3 + 2 * x1 + e
(dat <- data.frame(y=y, x1=x1, x2=x2, x3=x3, x4=x4))

```

1) Which variables do you expect 'y' to associated with? Which variables do you expect 'y' to be
   independent of? Plot all the variables in 'dat' against one another. Make point estimates of the 
   Pearson correlations of the variables with one another.

plot(dat)
cor(dat)

2) Use the data to fit a linear model to the formula 'y ~ x1'. What is the R-squared? How about the
   adjusted R-squared?

fit <- lm(y ~ x1, data=dat)
(smry <- summary(fit))
smry$r.squared
smry$adj.r.squared

3) Use the data to fit a linear model to the following formulas sequentially, comparing the R-squared 
   and adjusted R-squared across the fits: 'y ~ x1 + x2', 'y ~ x1 + x2 + x3', and 'y ~ x1 + x2 + x3 + x4'. 
   Does it seem like there is overfitting going on? Is the adjusted R-squared properly adjusting to 
   compensate for the overfitting?

fit <- lm(y ~ x1 + x2, data=dat)
smry <- summary(fit)
smry$r.squared
smry$adj.r.squared

fit <- lm(y ~ x1 + x2 + x3, data=dat)
smry <- summary(fit)
smry$r.squared
smry$adj.r.squared

fit <- lm(y ~ x1 + x2 + x3 + x4, data=dat)
smry <- summary(fit)
smry$r.squared
smry$adj.r.squared


###################################################################################################
## Lesson 4 : Check 3

1) Using the 'iris' dataset, fit an intercept-only model to the data with 'Sepal.Length' as the
   response variable.

rm(list=ls())
fit.lo <- lm(Sepal.Length ~ 1, data=iris)

2) Use the same data to fit a model with 'Sepal.Length' as response, and all the other variables
   as well as two-way interactions between those variables as predictors. 

fit.up <- lm(Sepal.Length ~ .^2, data=iris)

3) Use the R 'step()' function to explore the formula space between the intercept model and
   the model with all two-way interactions. Extract the formula for the final model.

fit <- step(fit.lo, scope=list(lower=fit.lo, upper=fit.up), direction='both', trace=1)
formula(fit)

4) Set up a 10-fold cross-validation, with five repetitions to estimate the performance 
   (mean-squared error) expected for the final model.

f <- function(idx.trn) {
  dat.trn <- iris[idx.trn, ]
  dat.tst <- iris[-idx.trn, ]
  fit.lo <- lm(Sepal.Length ~ 1, data=dat.trn)
  fit.hi <- lm(Sepal.Length ~ .^2, data=dat.trn)
  fit <- step(fit.lo, scope=list(lower=fit.lo, upper=fit.up), direction='both', trace=0)
  pred.trn <- predict(fit, newdata=dat.trn)
  pred.tst <- predict(fit, newdata=dat.tst)
  pred.int <- predict(fit.lo, newdata=dat.tst)
  mse.trn <- mean((dat.trn$Sepal.Length - pred.trn) ^ 2)
  mse.tst <- mean((dat.tst$Sepal.Length - pred.tst) ^ 2)
  mse.int <- mean((dat.tst$Sepal.Length - pred.int) ^ 2)
  c(mse.trn=mse.trn, mse.tst=mse.tst, mse.int=mse.int)
}

set.seed(1)
idx <- 1:nrow(iris)
folds <- caret::createMultiFolds(idx, k=10, times=5)
rslt <- sapply(folds, f)
apply(rslt, 1, mean)
apply(rslt, 1, sd)


###################################################################################################
## Lesson 5 : Check 1

1) Copy the `Species` and `Petal.Length` columns from `iris` dataset into a new data.frame
   called `dat`. Introduce a column `dat$Grp` and use it to encode non-virginica species as 'negative' 
   and 'virginica' as 'positive'. Delete the column `dat$Species`.

rm(list=ls())
dat <- iris[, c('Species', 'Petal.Length')]
dat$Grp <- 'negative'
dat$Grp[dat$Species == 'virginica'] <- 'positive'
dat$Grp <- factor(dat$Grp)
dat$Species <- NULL

2) Write a function that will take an integer index to a training set, such as that returned by
   `caret::createMultiFolds()`, splits `dat` into a training-set and test-set, fits a logistic 
   regression model to the formula `Grp ~ Petal.Length` using the training-set, then
   estimates the AUC for the model using the test-set. 

library('pROC')

f.cv <- function(idx.trn) {

  dat.trn <- dat[idx.trn, ]
  dat.tst <- dat[-idx.trn, ]

  fit <- glm(Grp ~ Petal.Length, data=dat.trn, family='binomial')

  pred.prob <- predict(fit, newdata=dat.tst, type='response')
  tmp <- pROC::roc(dat.tst$Grp, pred.prob, direction='<')

  as.numeric(tmp$auc)
}

3) Use your function to conduct a 10-fold cross-validation repeated 7 times. Report the mean
   and standard deviation of the AUC across the folds.

library('caret')
set.seed(1)
idx <- 1:nrow(dat)
folds <- caret::createMultiFolds(idx, k=10, times=7)
rslts <- sapply(folds, f.cv)
mean(rslts)
sd(rslts)

###################################################################################################
## Lesson 5 : Check 2

1) Using the `warpbreaks` Poisson regression example as a guide (copy and paste) make a function
   that will take an integer observation index, use it to split the `warpbreaks` data into a 
   training-set and test-set, use the `step()` function to explore **Poisson regressions** over the 
   formula space between the lower model `breaks ~ 1` and the upper model `breaks ~ .^2`, using 
   the lower model as the starting point. Then make predictions for both the training set and 
   test-set. Return performance estimates (the MSE) based on the training-set and on the test-set.

rm(list=ls())

f.cv <- function(idx.trn) {

  dat.trn <- warpbreaks[idx.trn, ]
  dat.tst <- warpbreaks[-idx.trn, ]   ## since idx.trn is integer index, use '-' to negate

  ## fit upper and lower models; select working model by stepping guided by AIC:
  fit.lo <- glm(breaks ~ 1, data=dat.trn, family='poisson')
  fit.up <- glm(breaks ~ .^2, data=dat.trn, family='poisson')
  fit <- step(fit.lo, scope=list(lower=fit.lo, upper=fit.up), direction='both', trace=0)

  prd.trn <- predict(fit, newdata=dat.trn, type='response')
  prd.tst <- predict(fit, newdata=dat.tst, type='response')
  prd.int <- predict(fit.lo, newdata=dat.tst, type='response')

  mse.trn <- mean((dat.trn$breaks - prd.trn) ^ 2)
  mse.tst <- mean((dat.tst$breaks - prd.tst) ^ 2)
  mse.int <- mean((dat.tst$breaks - prd.int) ^ 2)

  c(mse.trn=mse.trn, mse.tst=mse.tst, mse.int=mse.int)
}

2) Use your function to conduct a 10-fold cross-validation repeated 7 times. Report the mean
   and standard deviation of the MSE across the folds.

library('caret')
set.seed(1)
idx <- 1:nrow(warpbreaks)
folds <- caret::createMultiFolds(idx, k=10, times=7)
rslts <- sapply(folds, f.cv)
apply(rslts, 1, mean)
apply(rslts, 1, sd)

###################################################################################################
## Lesson 5 : Check 3

1) Using the `warpbreaks` NB regression example as a guide (copy and paste) make a function
   that will take an integer observation index, use it to split the `warpbreaks` data into a 
   training-set and test-set, use the `step()` function to explore **Negative binomial regression** 
   over the formula space between the lower model `breaks ~ 1` and the upper model `breaks ~ .^2`, 
   using the lower model as the starting point. Then make predictions for both the training set and 
   test-set. Return performance estimates (the MSE) based on the training-set and on the test-set.

library('MASS')
rm(list=ls())

f.cv <- function(idx.trn) {

  dat.trn <- warpbreaks[idx.trn, ]
  dat.tst <- warpbreaks[-idx.trn, ]   ## since idx.trn is integer index, use '-' to negate

  ## fit upper and lower models; select working model by stepping guided by AIC:
  fit.lo <- glm.nb(breaks ~ 1, data=dat.trn)
  fit.up <- glm.nb(breaks ~ .^2, data=dat.trn)
  fit <- step(fit.lo, scope=list(lower=fit.lo, upper=fit.up), direction='both', trace=0)

  prd.trn <- predict(fit, newdata=dat.trn, type='response')
  prd.tst <- predict(fit, newdata=dat.tst, type='response')
  prd.int <- predict(fit.lo, newdata=dat.tst, type='response')

  mse.trn <- mean((dat.trn$breaks - prd.trn) ^ 2)
  mse.tst <- mean((dat.tst$breaks - prd.tst) ^ 2)
  mse.int <- mean((dat.tst$breaks - prd.int) ^ 2)

  c(mse.trn=mse.trn, mse.tst=mse.tst, mse.int=mse.int)
}

2) Use your function to conduct a 10-fold cross-validation repeated 7 times. Report the mean
   and standard deviation of the MSE across the folds.

library('caret')
set.seed(1)
idx <- 1:nrow(warpbreaks)
folds <- caret::createMultiFolds(idx, k=10, times=7)
rslts <- sapply(folds, f.cv)
apply(rslts, 1, mean)
apply(rslts, 1, sd)


