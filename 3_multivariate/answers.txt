Multivariate statistics

###################################################################################################
## Lesson 1 : Check 1 

## 1) Fit a linear model using `lm()` with `trees$Volume` as response and
##    `trees$Height` using the 'optimal' `lambda` value suggested above (instead
##    of rounding `lambda` to `0` and using the `log()`) to transform the 
##    response. How do the residual plots differ from using the `log()`?

rm(list=ls())

fit1 <- lm(Volume ~ Height, data=trees)
out <- boxcox(fit1)
(lambda.best <- out$x[which.max(out$y)])

fit2 <- lm(log(Volume) ~ Height, data=trees)

dat <- trees
dat$Volume <- dat$Volume ^ lambda.best
fit3 <- lm(Volume ~ Height, data=dat)

par(mfrow=c(2, 3))
plot(fit2, which=1:6)             ## homoskedastic, but not very normal

dev.new()                         ## open new plotting window
par(mfrow=c(2, 3)) 
plot(fit3, which=1:6)             ## pretty similar to fit2, but hard to interpret

dev.off()                         ## close extra plotting window


## 2) Fit another linear model with `lm()` with `trees$Volume` as response and
##    `trees$Height` trying a `lambda` value of `-1` (corresponding to the 
##    reciprocal), to transform the response. How do the residual plots 
##    differ from using the `log()`?

rm(list=ls())

fit1 <- lm(log(Volume) ~ Height, data=trees)

dat <- trees
dat$Volume <- dat$Volume ^ -1
fit2 <- lm(Volume ~ Height, data=dat)

par(mfrow=c(2, 3))
plot(fit1, which=1:6)             ## homoskedastic, but not very normal

dev.new()                         ## open new plotting window
par(mfrow=c(2, 3)) 
plot(fit2, which=1:6)             ## more normal, but maybe heteroskedastic; outlier influence larger

dev.off()                         ## close extra plotting window


###################################################################################################
## Lesson 1 : Check 2 

## 1. Repeat the simulation of position vs time of an accelerating system from above (copy and
##    paste). Then split the data into 20% test set and 80% training set (copy and paste). 
##    Then fit a model of `y ~ tm` to the training data. Then fit another model of `sqrt(y) ~ tm` to 
##    the training data. Finally, fit a model of `y ~ I(tm ^ 2)`.

rm(list=ls())
set.seed(1)

# simulate data:
n <- 100                          ## sample size
p.tst <- 0.2                      ## proportion for test set
n.tst <- round(p.tst * n)         ## test set size
accel <- 3                        ## acceleration
y.init <- 1000                    ## initial position
tm <- runif(n, min=0, max=n)      ## time
y.mdl <- y.init + 0.5 * accel * (tm ^ 2)
err <- rnorm(length(tm), mean=0, sd=150)
dat <- data.frame(y=y.mdl + err, tm=tm)

# split into training and test sets:
idx.tst <- sample(1 : n, n.tst, replace=F)
i.tst <- rep(F, n)
i.tst[idx.tst] <- T
dat.tst <- dat[i.tst, ]
dat.trn <- dat[! i.tst, ]

# fit the models:
fit0 <- lm(y ~ tm, data=dat.trn)
fit1 <- lm(sqrt(y) ~ tm, data=dat.trn)
fit2 <- lm(y ~ I(tm ^ 2), data=dat.trn)

## 2. Generate residual plots for all three fits and rank the fits with regard to how well
##    you feel they meed assumptions for the linear model.

# plot the residuals:
par(mfrow=c(2, 3))
plot(fit0, which=1:6)             ## pretty bad
plot(fit1, which=1:6)             ## better, but not so great
plot(fit2, which=1:6)             ## much better

## 3. Make predictions of the response values for the observations in the test set. Generate
##    estimates of the mean-squared-error for each fit based on the test set.

# predictions for fits: 
pred0 <- predict(fit0, newdata=dat.tst)
pred1 <- predict(fit1, newdata=dat.tst)
pred1 <- pred1 ^ 2                ## MUST UNDO TRANSFORMATION!!!
pred2 <- predict(fit2, newdata=dat.tst)

# prediction error:
mean((dat.tst$y - pred0) ^ 2)
mean((dat.tst$y - pred1) ^ 2)
mean((dat.tst$y - pred2) ^ 2)

## 4. Plot the predicted response values on the y/left/vertical axis and observed response values on the
##    x/bottom/horizontal axis for all three fits.

par(mfrow=c(1, 3))

plot(x=dat.tst$y, y=pred0, main="y ~ tm")
abline(a=0, b=1)

plot(x=dat.tst$y, y=pred1, main="sqrt(y) ~ tm")
abline(a=0, b=1)

plot(x=dat.tst$y, y=pred2, main="y ~ I(tm ^ 2)")
abline(a=0, b=1)


###################################################################################################
## Lesson 1 : Check 3 

## 1. Split the built-in 'DNase' dataset into a test-set consisting of `Run == 3` and `Run == 6` and a 
##    training-set consisting of the other nine `Run` values.

i.tst <- DNase$Run %in% c(3, 6)
dat.tst <- DNase[i.tst, ]
dat.trn <- DNase[! i.tst, ]
summary(dat.tst$Run)              ## looks right
summary(dat.trn$Run)              ## looks right

## 2. Fit a linear model of `density ~ conc` to the training data. Make predictions on the held-out test-set.

fit.lm <- lm(density ~ conc, data=dat.trn)
pred.lm <- predict(fit.lm, newdata=dat.tst)

## 3. Fit a loess model to the same formula, using the default parameter settings. Make predictions on the
##    held-out test-set.

fit.loess <- loess(density ~ conc, data=dat.trn)
pred.loess <- predict(fit.loess, newdata=dat.tst)

## 4. Calculate the mean-squared error for the two models based on the test-set predictions.

mean((dat.tst$density - pred.lm) ^ 2)
mean((dat.tst$density - pred.loess) ^ 2)


###################################################################################################
## Lesson 2 : Check 1

1. From the built-in `warpbreaks` dataset, split out the `breaks` values where `wool == 'A'` 
   and `tension == 'L'` as one variable (say `x`). Split out the `breaks` values where 
   `wool == 'B'` and `tension == 'M'` as another variable (say `y`). 

rm(list=ls())

dat <- warpbreaks
i.x <- dat[, 'wool'] == 'A' & dat[, 'tension'] == 'L'
i.y <- dat[, 'wool'] == 'B' & dat[, 'tension'] == 'M'

x <- dat[i.x, 'breaks']
y <- dat[i.y, 'breaks']
table(dat[, 'wool'], dat[, 'tension'])
length(x)
length(y)

2. Rearrange the data to be in a data-frame with `length(x) + length(y)` rows and two columns. The 
   first column should be the `breaks` and the second column should be a new indicator, of type
   `character` or `factor`, representing group membership. The first group (maybe labeled 'A') is
   the data with `wool == 'A'` and `tension == 'L'`, and the other (maybe 'B') is the data with
   `wool == 'B'` and `tension == 'M'`. Hint: concatenate `x` and `y` to make the first column; 
   use their lengths and `rep()` to specify the group variable.

grps = c(rep('A', length(x)), rep('B', length(y)))
(dat <- data.frame(x=c(x, y), grp=grps))

3. Conduct a Bartlett test to see if the variances between the two groups is different.

bartlett.test(x=c(x, y), g=grps)

4. We will assume the Bartlett test showed no evidence of unequal variances, so conduct a 
   two-sample equal variances t-test on the null hypothesis that the mean `breaks` of 
   group `A` and group `B` (or equivalently, `x` and `y`) are different. Return the 
   statistic (t-statistic in this case) and p-value from the test.

fit <- t.test(x=x, y=y, var.equal=T)
fit$statistic
fit$p.value

5. Conduct 9999 permutations of group labels, conducting the same `t.test()` described 
   above, saving the statistic for each iteration. Compare the distribution of permutation
   results to the original unpermuted result in order to calculate a permutation p-value. 
   Does it corroborate or contradict the parametric test? Hint: this will likely be 
   easier if you use the data.frame you created above and permute the group labels.

set.seed(1)                       ## make pseudo-random process reproducible
R <- 9999                         ## number of permutations
rslts <- rep(as.numeric(NA), R)   ## to hold results; pre-extended for efficiency

for(i in 1 : R) {
  dat.i <- dat
  dat.i$grp <- sample(dat.i$grp, nrow(dat.i), replace=F)
  x.i <- dat.i$x[dat.i$grp == 'A']
  y.i <- dat.i$x[dat.i$grp == 'B']
  fit.i <- t.test(x=x.i, y=y.i, var.equal=T)
  rslts[i] <- fit.i$statistic
}

fit$statistic                     ## test statistic from non-permuted (original) data
summary(rslts)                    ## should be no NAs: if there are, need to adjust for them
(n.exceed <- sum(abs(rslts) >= abs(fit$statistic)))
(n.exceed + 1) / (R + 1)          ## permutation p-value (should never be 0!!!)
fit$p.value


###################################################################################################
## Lesson 2 : Check 2

1. Fit a linear model with the formula `sqrt(Volume) ~ Girth` to the built-in trees dataset.

rm(list=ls())
fit <- lm(sqrt(Volume) ~ Girth, data=trees)
summary(fit)
par(mfrow=c(2, 3))
plot(fit, which=1:6)

2. Generate a parametric 95% confidence interval for the for the Girth coefficient.

confint(fit)['Girth', ]           ## the parametric confidence interval

3. Generate a bootstrap 95% confidence interval (BCa if it works, otherwise Percentile) on the
   Girth coefficient. Does it generally corroborate or contradict the parametric interval?

library(boot)
set.seed(1)

## boot() needs function taking data and integer index, returning estimate:

f <- function(dat, i) {
  fit.i <- lm(sqrt(Volume) ~ Girth, data=dat[i, ])
  coef(fit.i)['Girth']
}

f(trees, rep(T, nrow(trees)))     ## check: should return the same estimate as the original fit

R <- 999                          ## number of bootstrap iterations
out <- boot(trees, f, R)          ## the bootstrapping
par(mfrow=c(1, 1))
plot(out)                         ## bias? skew? normal? 
jack.after.boot(out)              ## outliers affect point estimate or CIs?
(ci <- boot.ci(out))              ## bootstrap confidence intervals; BCa preferred
confint(fit)['Girth', ]           ## the parametric confidence interval

###################################################################################################
## Lesson 2 : Check 3

1. Fit a linear model to the formula `Volume ~ Girth` for the built-in `trees` dataset. Fit a 
   second linear model to the formula `sqrt(Volume) ~ Girth`. Generate summaries for each fit.

rm(list=ls())

fit1 <- lm(Volume ~ Girth, data=trees)
fit2 <- lm(sqrt(Volume) ~ Girth, data=trees)
summary(fit1)
summary(fit2)

2) Use 5-fold cross-validation with 3 repetitions (use the `caret::createMultiFolds()` parameter 
   `times` to set the repetitions) to compare the two formulas above in terms of mean-squared 
   error of the resulting models. Examine the mean and standard deviations of the results 
   for each formula to decide if the `sqrt()` transformation is worthwhile. 

library('caret')
set.seed(1)

k <- 5
times <- 3
dat <- trees
frm1 <- Volume ~ Girth
frm2 <- sqrt(Volume) ~ Girth

f <- function(idx) {

  ## split into training and testing:
  dat.trn <- dat[idx, ]
  dat.tst <- dat[-idx, ]

  ## fit Volume ~ Girth:
  fit1 <- lm(frm1, data=dat.trn)
  pred1 <- predict(fit1, newdata=dat.tst)

  ## fit sqrt(Volume) ~ Girth:
  fit2 <- lm(frm2, data=dat.trn)
  pred2 <- predict(fit2, newdata=dat.tst)
  pred2 <- pred2 ^ 2

  ## estimate error for each model:
  mse1 <- mean((dat.tst$Volume - pred1) ^ 2, na.rm=T)
  mse2 <- mean((dat.tst$Volume - pred2) ^ 2, na.rm=T)

  ## return error estimates:
  c(mse.lm=mse1, mse.loess=mse2)
}

idx <- 1 : nrow(dat)
folds <- createMultiFolds(idx, k=k, times=times)

rslt <- sapply(folds, f)
apply(rslt, 1, mean)
apply(rslt, 1, sd)

###################################################################################################
## Lesson 3 : Check 1

rm(list=ls())

dat <- mtcars[, c('mpg', 'wt', 'disp', 'hp')]
par(mfrow=c(1, 1))
plot(dat)
cor(dat)

fit1 <- lm(mpg ~ disp, data=dat)
summary(fit1)
par(mfrow=c(2, 3))
plot(fit1, which=1:6)

fit2 <- lm(mpg ~ disp + hp, data=dat)
summary(fit2)
par(mfrow=c(2, 3))
plot(fit2, which=1:6)

f <- function(idx.trn) {
  dat.trn <- dat[idx.trn, ]
  dat.tst <- dat[-idx.trn, ]
  fit1 <- lm(mpg ~ disp, data=dat.trn)
  fit2 <- lm(mpg ~ disp + hp, data=dat.trn)
  prd1 <- predict(fit1, newdata=dat.tst)
  prd2 <- predict(fit2, newdata=dat.tst)
  mse1 <- mean((dat.tst$mpg - prd1) ^ 2, na.rm=T)
  mse2 <- mean((dat.tst$mpg - prd2) ^ 2, na.rm=T)
  mse1 - mse2                     ## will produce more precise comparison
}

library(caret)
set.seed(1)

k <- 10
times <- 7
idx <- 1:nrow(dat)

## make list of k * times folds; elements are training-set indices:
folds <- createMultiFolds(idx, k=k, times=times)

rslt <- sapply(folds, f)          ## run f() on each training-set index in folds
mean(rslt)
sd(rslt)

###################################################################################################
## Lesson 3 : Check 2

correlation stuff here

###################################################################################################
## Lesson 3 : Check 3

rm(list=ls())

dat <- mtcars[, c('mpg', 'wt', 'disp', 'hp')]
par(mfrow=c(1, 1))
plot(dat)
cor(dat)

fit1 <- lm(mpg ~ disp + hp, data=dat)
summary(fit1)
par(mfrow=c(2, 3))
plot(fit1, which=1:6)

fit2 <- lm(mpg ~ disp * hp, data=dat)
summary(fit2)
par(mfrow=c(2, 3))
plot(fit2, which=1:6)

f <- function(idx.trn) {
  dat.trn <- dat[idx.trn, ]
  dat.tst <- dat[-idx.trn, ]
  fit1 <- lm(mpg ~ disp + hp, data=dat.trn)
  fit2 <- lm(mpg ~ disp * hp, data=dat.trn)
  prd1 <- predict(fit1, newdata=dat.tst)
  prd2 <- predict(fit2, newdata=dat.tst)
  mse1 <- mean((dat.tst$mpg - prd1) ^ 2, na.rm=T)
  mse2 <- mean((dat.tst$mpg - prd2) ^ 2, na.rm=T)
  mse1 - mse2                     ## will produce more precise comparison
}

library(caret)
set.seed(1)

k <- 10
times <- 7
idx <- 1:nrow(dat)

## make list of k * times folds; elements are training-set indices:
folds <- createMultiFolds(idx, k=k, times=times)

rslt <- sapply(folds, f)          ## run f() on each training-set index in folds
mean(rslt)
sd(rslt)


